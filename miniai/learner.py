# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_learner.ipynb.

# %% auto 0
__all__ = ['DataLoaders', 'to_cpu', 'identity', 'CancelFitException', 'CancelBatchException', 'CancelEpochException', 'with_cbs',
           'Learner', 'Callback', 'run_cbs', 'SingleBatchCB', 'TrainCB', 'DeviceCB', 'MetricsCB', 'ProgressCB',
           'TrainLearner', 'MomentumLearner', 'LRFinderCB', 'lr_find']

# %% ../nbs/09_learner.ipynb 2
import math,torch,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy

from torch import optim
import torch.nn.functional as F

from .conv import *
from .datasets import *

from fastprogress import progress_bar,master_bar

# %% ../nbs/09_learner.ipynb 4
from torch.utils.data import DataLoader

# %% ../nbs/09_learner.ipynb 9
class DataLoaders:
    def __init__(self, *dls):
        self.train, self.valid = dls[0], dls[1]

    @classmethod
    def from_dd(cls, dd, batch_size, as_tuple=True, num_workers=4):
        return cls(*[
            DataLoader(
                ds, batch_size=batch_size,
                collate_fn=collate_dict(ds), num_workers=num_workers
            ) for ds in dd.values()
        ])

# %% ../nbs/09_learner.ipynb 23
from torcheval.metrics import MulticlassAccuracy, Mean

# %% ../nbs/09_learner.ipynb 28
def to_cpu(x):
    if isinstance(x, torch.Tensor):
        return x.detach().cpu()
    if isinstance(x, Mapping):
        return {k: to_cpu(v) for k, v in x.items()}
    return type(x)(to_cpu(i) for i in x)

# %% ../nbs/09_learner.ipynb 30
def identity(*args):
    return (*args,)

# %% ../nbs/09_learner.ipynb 36
class CancelFitException(Exception): ...
class CancelBatchException(Exception): ...
class CancelEpochException(Exception): ...

# %% ../nbs/09_learner.ipynb 37
class with_cbs:
    def __init__(self, nm):
        self.nm = nm
        self.before = f"before_{nm}"
        self.after = f"after_{nm}"
        self.exception = f"Cancel{nm.title()}Exception"
        self.cleanup = f"cleanup_{nm}"

    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(self.before)
                f(o, *args, **kwargs)
                o.callback(self.after)
            except globals()[self.exception]: ...
            finally: o.callback(self.cleanup)
        return _f
                

# %% ../nbs/09_learner.ipynb 38
class Learner:
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):
        if cbs is None:
            cbs = []
        fc.store_attr()
        for cb in cbs:
            cb.learn = self

    @with_cbs("batch")
    def _one_batch(self):
        self.predict()
        self.callback("after_predict")
        self.get_loss()
        self.callback("after_loss")
        if self.model.training:
            self.backward()
            self.callback("after_backward")
            self.step()
            self.callback("after_step")
            self.zero_grad()

    def one_epoch(self, train):
        self.model.train(train)
        if train:
            self.dl = self.dls.train
        else:
            self.dl = self.dls.valid

        self._one_epoch()

    @with_cbs("epoch")
    def _one_epoch(self):
        for self.batch_idx, self.batch in enumerate(self.dl):
            self._one_batch()

    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):
        if cbs is None:
            cbs = []
        for cb in cbs:
            self.cbs.append(cb)
        try:
            self.n_epochs = n_epochs
            self.range_epochs = range(self.n_epochs)
            if lr is None:
                lr = self.lr
            if self.opt_func:
                self.opt = self.opt_func(self.model.parameters(), lr)
            self._fit(train, valid)
        finally:
            for cb in cbs:
                self.cbs.remove(cb)

    @with_cbs("fit")
    def _fit(self, train, valid):
        for self.epoch in self.range_epochs:
            if train: self.one_epoch(True)
            if valid: self.one_epoch(False)

    def __getattr__(self, name):
        if name in ("predict", "get_loss", "backward", "step", "zero_grad"):
            return partial(self.callback, name)
        raise AttributeError(name)
    
    def callback(self, method_nm):
        run_cbs(self.cbs, method_nm, self)

    @property
    def training(self):
        return self.model.training

# %% ../nbs/09_learner.ipynb 39
class Callback:
    order = 0

# %% ../nbs/09_learner.ipynb 40
def run_cbs(cbs, method_nm, learn=None):
    for cb in sorted(cbs, key=lambda x: x.order):
        method = getattr(cb, method_nm, identity)
        if method: method(learn)

# %% ../nbs/09_learner.ipynb 41
class SingleBatchCB(Callback):
    def after_batch(self, learn): raise CancelFitException()

# %% ../nbs/09_learner.ipynb 42
class TrainCB(Callback):
    def __init__(self, n_inp=1): self.n_inp = n_inp

    def predict(self, learn):
        learn.preds = learn.model(*learn.batch[:self.n_inp])

    def get_loss(self, learn):
        learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])

    def backward(self, learn):
        learn.loss.backward()

    def step(self, learn):
        learn.opt.step()

    def zero_grad(self, learn):
        learn.opt.zero_grad()

# %% ../nbs/09_learner.ipynb 43
class DeviceCB(Callback):
    def __init__(self, device=def_device): self.device = def_device
    def before_fit(self, learn):
        if hasattr(learn.model, "to"):
            learn.model.to(self.device)
    def before_batch(self, learn):
        learn.batch = to_device(learn.batch, self.device)

# %% ../nbs/09_learner.ipynb 45
class MetricsCB(Callback):
    def __init__(self, *args, **kwargs):
        self.metrics = kwargs
        for arg in args:
            self.metrics[type(arg).__name__] = arg

        self.loss = Mean()
        self.all = self.metrics.copy()
        self.all["loss"] = self.loss

    def log(self, to_log):
        print(to_log) 

    def before_fit(self, learn): learn.metrics = self

    def before_epoch(self, learn):
        for m in self.all.values():
            m.reset()

    def after_epoch(self, learn):
        to_log = {name: f"{m.compute():.3f}" for name, m in self.all.items()}
        to_log["epoch"] = learn.epoch
        to_log["train"] = "train" if learn.model.training else "valid"
        self.log(to_log)

    def after_batch(self, learn):
        batch = to_cpu(learn.batch) 
        x, y = batch[0], batch[1]
        while isinstance(x, tuple): x = x[0]  # TODO: this is quite bad
        preds = to_cpu(learn.preds)
        loss = to_cpu(learn.loss)
        for m in self.metrics.values():
            m.update(preds, y)
        self.loss.update(loss, weight=x.shape[0])

# %% ../nbs/09_learner.ipynb 51
class ProgressCB(Callback):
    order = MetricsCB.order + 1
    def __init__(self, plot=False, update_rate=100): fc.store_attr()

    def before_fit(self, learn):
        learn.range_epochs = master_bar(learn.range_epochs)
        self.mb = learn.range_epochs
        self.first = True
        if hasattr(learn, "metrics"):
            learn.metrics.log = self.log
        self.losses = []
        self.val_losses = []

    def log(self, to_log):
        if self.first:
            self.first = False
            self.mb.write(list(to_log), table=True)    
        self.mb.write(list(to_log.values()), table=True)

    def before_epoch(self, learn):
        learn.dl = progress_bar(learn.dl, leave=False, parent=self.mb)

    def after_batch(self, learn):
        learn.dl.comment = f"{learn.loss:.4f}"
        if self.plot and hasattr(learn, "metrics") and learn.model.training:
            self.losses.append(to_cpu(learn.loss).item())
            if (learn.batch_idx % self.update_rate) == 0:
                self._update_graph(learn)

    def after_epoch(self, learn):
        if not learn.training:
            self.val_losses.append(learn.metrics.loss.compute())
            self._update_graph(learn)
            
    def _update_graph(self, learn):
        to_update = [[list(range(len(self.losses))), self.losses]]
        if self.val_losses:
            to_update.append([list(map(lambda step: (step+1)*len(learn.dls.train), range(len(self.val_losses)))), self.val_losses])
        self.mb.update_graph(to_update)

# %% ../nbs/09_learner.ipynb 54
class TrainLearner(Learner):
    def predict(self):
        self.preds = self.model(self.batch[0])

    def get_loss(self):
        self.loss = self.loss_func(self.preds, self.batch[1])

    def backward(self):
        self.loss.backward()

    def step(self):
        self.opt.step()

    def zero_grad(self):
        self.opt.zero_grad()

# %% ../nbs/09_learner.ipynb 55
class MomentumLearner(TrainLearner):
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD, mom=0.85):
        self.mom = mom
        super().__init__(model, dls, loss_func, lr, cbs, opt_func)
    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters():
                p.grad *= 0.8

# %% ../nbs/09_learner.ipynb 62
from torch.optim.lr_scheduler import ExponentialLR

# %% ../nbs/09_learner.ipynb 64
class LRFinderCB(Callback):
    def __init__(self, mult=1.3, threshold=3.0): fc.store_attr()

    def before_fit(self, learn):
        self.lrs = []
        self.losses = []
        self.min = float("inf")
        self.sched = ExponentialLR(learn.opt, self.mult)

    def after_batch(self, learn):
        if not learn.model.training:
            raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]["lr"])
        loss = to_cpu(learn.loss).item()
        self.losses.append(loss)
        self.min = min(loss, self.min)
        if math.isnan(loss) or loss > self.min * self.threshold:
            raise CancelFitException()
        self.sched.step()

    def cleanup_fit(self, learn):
        plt.plot(self.lrs, self.losses)
        plt.xscale("log")

# %% ../nbs/09_learner.ipynb 66
@fc.patch
def lr_find(self:Learner, mult=1.3, threshold=3.0, start_lr=1e-5, max_epochs=10):
    self.fit(max_epochs, lr=start_lr, cbs=[LRFinderCB(mult=mult, threshold=threshold)])
